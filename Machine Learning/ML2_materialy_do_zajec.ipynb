{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metoda K Najbliższych sąsiadów (KNN)\n",
    "\n",
    "<img src=\"Grafika/knn.png\",width=\"500\">\n",
    "\n",
    "Źródło: https://i.pinimg.com/originals/65/36/b9/6536b9a63fc427e0fc3e1a9687b49aff.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "?KNeighborsClassifier\n",
    "\n",
    "# argument - _weights_ umozliwia bardziej zlozony schemat klasyfikacji niz klasyfikacja wiekszosciowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikator SVM - Support Vector Machines\n",
    "\n",
    "*W sklearn: SVC.* \n",
    "*Po polsku: maszyna wektorów nośnych.*\n",
    "\n",
    "<img src=\"Grafika/svm_intro1.jpg\"style=\"width: 200px;\">\n",
    "<img src=\"Grafika/svm_intro2.jpg\"style=\"width: 300px;\">\n",
    "Źródło: https://www.safaribooksonline.com/library/view/python-deeper-insights/9781787128576/ch03s04.html\n",
    "\n",
    "### Matematyka\n",
    "\n",
    "Zajmujemy się klasyfikacją binarną, ale przyjmujemy konwencję $Y\\in \\{-1,1\\}$. \n",
    "\n",
    "Rozważmy przypadek, gdy dane są liniowo separowalne.\n",
    "\n",
    "$ \\mathbf{w} \\cdot \\mathbf{x} = 0$ - równanie opisujące linię (hiperpłaszczyznę) rozdzielającą;\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "  \\left.\\begin{aligned}\n",
    "  \\mathbf{w} \\cdot \\mathbf{x} - b & = 1\\\\\n",
    "  \\mathbf{w} \\cdot \\mathbf{x} - b & = -1\n",
    "\\end{aligned}\\right\\} \\text{- równania marginesów.}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Zatem SVM szuka takiej płaszczyzny (parametrów $\\mathbf{w}, b$), dla której:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "  \\mathbf{w} \\cdot x_i - b & \\geq 1, \\ \\ \\text{ gdy } \\ y_i = 1, \\\\\n",
    "  \\mathbf{w} \\cdot x_i - b & \\leq -1 \\ \\ \\text{ gdy } \\ y_i = -1,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Co można zapisać w skrócie warunkiem:\n",
    "\n",
    "$y_i ( \\mathbf{w} \\cdot x_i - b) \\geq 1.$\n",
    "\n",
    "A ponieważ odległość między marginesami wynosi $\\frac{2}{\\|\\mathbf{w}\\|}$, to ostatecznie uczenie klasyfikatora SVM można zdefiniować jako:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "  \\text{zminimalizuj } & \\ \\ \\ \\|\\mathbf{w}\\|, \\\\ \n",
    "  \\text{przy ograniczeniu: } & \\ \\ \\ y_i ( \\mathbf{w} \\cdot x_i - b) \\geq 1, \\ \\ \\ i = 1, \\ldots, n.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## Dane nieseparowalne \n",
    "\n",
    "\n",
    "<img src=\"Grafika/svm_nonlinear0.png\"style=\"width: 500px;\">\n",
    "Źródło: http://inspirehep.net/record/1265323/plots\n",
    "\n",
    "Definiujemy zawiasową funkcję straty (*hinge loss*):\n",
    "\n",
    "$\\zeta _{i} = \\max{(0, 1 - y_i ( \\mathbf{w} \\cdot x_i - b))}.$\n",
    "\n",
    "Funkcja ta przyjmuje wartość $0$, gdy obserawcja $x_i$ leży po właściwej stronie hiperpłaszczyzny rozdzielającej, oraz jej wartość jest proporcjonalna do odległości do płaszczyzny w przypadku, gdy punkt leży po złej stronie.\n",
    "\n",
    "Uczenie klasyfikatora definiujemy jako minimalizacja funkcji:\n",
    "\n",
    "$$\\bigg[\\frac {1}{n}\\sum\\limits_{i=1}^{n}\\max (0,1-y_{i}({\\mathbf {w}}\\cdot {\\mathbf {x}}_{i}-b) )\\bigg] + \\lambda \\| {\\mathbf {w}}\\| ^{2}.$$\n",
    "\n",
    "Czyli jednocześnie maksymalizujemy odległość między marginesami (minimalizujemy $\\| {\\mathbf {w}}\\|$) oraz minimalizujemy karę za punkty leżące po złej stronie. $\\lambda$ - współczynnik ważący składowe opytmalizowanej funkcji.\n",
    "\n",
    "Uwaga: tak samo jak w regresji logistycznej w sklearn pojawia się parametr `C` - odpowiada on wartości $\\frac{1}{\\lambda}$. Zatem: im większe `C`, tym mniejszą wagę przywiązujemy do szerokości marginesu, a większą do położenia punktów po właściwych stronach, czyli margines będzie węższy ale lepiej dopasowany.\n",
    "\n",
    "<br>\n",
    "**Uwaga 1**: Czy SVM da nam prawdopodobieństwo przynależenia obserwacji do klas (jak $\\pi(x)$ w regresji logistycznej, czy empiryczny procent klas w liściu w drzewie decyzyjnym?\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Z definicji nie. Ale w praktyce się to robi. W uproszczeniu, przyjmuje się, że $P(Y=1 \\mid x) = \\text{sigmoid}(x) = \\frac{1}{1+e^{-d(x)}}$, gdzie $d(x)$ - odległość punktu $x$ od hiperpłaszczyzny rozdzielającej. W `sklearn` musimy ustawić w konstruktorze SVC parametr `probability=True`, jeśli chcemy żeby klasyfikator wyliczył te prawdopodobieństwa.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Uwaga 2**: Czy SVM można zastosować do klasyfikacji wieloklasowej?\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Tak jak w regresji logistycznej, klasyfikator jest zdefiniowany dla problemu klasyfikacji binarnej, ale w praktyce oczywiście jest na to sposób i to się robi. \n",
    "\n",
    "Można wykorzystać schemat *one-vs-rest*: w przypadku $K$ klas uczymy model $K$ razy do problemów binarnych - $Y = k$ vs $Y \\neq k$. Ostateczna predykcja, to klasa $c$, dla której prawdopodobieństwo $Y = c$ było największe. \n",
    "\n",
    "Inny schemat to *one-vs-one*. Uczymy rozwiązywać problem klasyfikacji dla wszystkich kombinacji dwóch klas. Predykcja to najcześciej wskazana klasa przez poszczególne klasyfikatory.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Uwaga 3**: Czy SVM wymaga jakiegoś przygotowania danych?\n",
    "\n",
    "Tak - skalowanie.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Kernel trick\n",
    "\n",
    "#### Jak można wykorzystać SVM do klasyfikacji w przypadku danych nieliniowych.\n",
    "\n",
    "<img src=\"Grafika/svm_nonlinear1.png\"style=\"width: 400px;\">\n",
    "\n",
    "Rozważmy rysunek B. W przedstawionych danych mamy dwie zmienne: $x_1, x_2$. Czy da się zastosować tutaj klasyfikator SVM?\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Rozszerzmy ręcznie zbiór danych do następujących zmiennych:\n",
    "\n",
    "$$x_1, x_2, x_1^2, x_2^2.$$\n",
    "\n",
    "Gdybyśmy do takich zmiennych zastosowali SVM na tym zbiorze, to jak mogłaby wyglądać płaszczyzna rozdzielająca? Na przykład tak:\n",
    "\n",
    "$$ 0 \\cdot x_1 + 0 \\cdot x_2 + a \\cdot x_1^2 + b \\cdot x_2^2 + c= 0.$$\n",
    "\n",
    "Czyli $\\mathbf{w} = [0, 0 , a, b]$.\n",
    "\n",
    "Wszystko fajnie, ale skąd mamy wiedzieć jakie przekształcenia zmiennych dodać (np. na rysunku poniżej - pomijając już nawet fakt, że w rzeczywistych danych nawet nie będziemy w stanie spojrzeć na jakikolwiek rysunek...)? Możemy dodać dla każdej zmiennej dużo różnych transformacji, ale jeśli zmiennych pierwotnie będzie dużo, to z transfromacjami będzie ich $\\text{dużo}^2$...\n",
    "\n",
    "<img src=\"Grafika/svm_nonlinear2.png\"style=\"width: 300px;\">\n",
    "\n",
    "\n",
    "Wróćmy do uczenia klasyfikatora, czyli rozwiązywania problemu optymalizacyjnego. \n",
    "\n",
    "Przyjmując oznaczenie $\\zeta _{i}=\\max (0,1-y_{i}(\\mathbf{w}\\cdot x_{i}-b))$, można go alternatywnie zapisać w postaci:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "  \\text{zminimalizuj } & \\ \\ \\ \\frac {1}{n}\\sum\\limits_{i=1}^{n} \\zeta_i + \\lambda \\|\\mathbf{w}\\|^2, \\\\ \n",
    "  \\text{przy ograniczeniu: } & \\ \\ \\ y_i ( \\mathbf{w} \\cdot x_i - b) \\geq 1 - \\zeta_i \\ \\text{oraz} \\ \\zeta_i \\geq 0, \\ \\ \\ i = 1, \\ldots, n.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "Okazuje się (matematyka wyższa...), że problem ten można sformułować równoważnie jako problem maksymalizacji funkcji:\n",
    "\n",
    "\n",
    "$$f(c_1 \\ldots c_n) = \\sum\\limits_{i=1}^n c_i - \\frac{1}{2}\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n y_i c_i (x_i\\cdot x_j) y_j c_j,\n",
    "$$\n",
    "\n",
    "przy pewnych ograniczeniach, dla pewnych $c_1, \\ldots, c_n$ (i tak się to w rzeczywistości odbywa). Widzimy zatem, że rozwiązanie zależy od iloczynów skalarnych obserwacji $x_i\\cdot x_j$.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Rozważmy transformacje zmiennych i iloczyny sklarne. Przyjmijmy, że mamy dwie zmienne oryginalne: $x_1, x_2$. Załóżmy, że transformujemy dane do $x_1^2, x_1x_2, x_2x_1, x_2^2$. Czyli mamy przekształcenie $f(x_1,x_2) = (x_1^2, x_1x_2, x_2x_1, x_2^2)$. Weźmy przykładowo dwie obserwacje $a = (a_1, a_2), b = (b_1, b_2)$. Obserwacje te w nowej przestrzeni mają postać $f(a) = (a_1^2, a_1 a_2, a_2 a_1, a_2^2), \\ f(b) = (b_1^2,b_1 b_2, b_2 b_1, b_2^2)$. Przeanalizujmy iloczyn skalarny:\n",
    "\n",
    "$f(a) \\cdot f(b) = a_1^2 \\cdot b_1^2 + a_1a_2 \\cdot b_1b_2 + a_2a_1 \\cdot b_2b_1 + a_2^2 \\cdot b_2^2 = (a_1b_1)^2 + 2a_1a_2b_1b_2 + (a_2b_2)^2$.\n",
    "\n",
    "Teraz rozważmy funkcję $K(z) = z^2$ i spójrzmy na wynik działania tej funkcji na iloczynie $a \\cdot b$:\n",
    "\n",
    "$K(a \\cdot b) = K(a_1 \\cdot b_1 + a_2 \\cdot b_2) = (a_1 \\cdot b_1 + a_2 \\cdot b_2)^2 = (a_1b_1)^2 + 2a_1a_2b_1b_2 + (a_2b_2)^2$.\n",
    "\n",
    "Wniosek?\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "$f(a) \\cdot f(b) = K(a \\cdot b)$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Zatem, zamiast transformować zmienne, wystarczy użyć pewnego przekształcenia $K(x_i \\cdot x_j)$ zamiast surowych iloczynów $x_i \\cdot x_j$.\n",
    "\n",
    "Dzięki temu:\n",
    "- koszt obliczeniowy jest dużo mniejszy.\n",
    "- pozbywamy się problemu szukania transformacji - w miejsce tego testujemy kilka przekształceń $K$.\n",
    "\n",
    "$K$ jest zwane **jądrem** (funkcją jądrową, ang. *kernel*).\n",
    "\n",
    "\n",
    "Jądra w sklearn:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/svm.html#svm-kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co SVM ma wspólnego z regresją logistyczną?\n",
    "\n",
    "### Reguła decyzyjna w SVM:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "  \\hat{y_i} =  & \\ \\ \\ \\ 1, \\ \\ \\text{ gdy }\\mathbf{w} \\cdot x_i  \\geq 0, \\\\\n",
    "  \\hat{y_i} = & - 1, \\ \\ \\text{ gdy }\\mathbf{w} \\cdot x_i  < 0.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "\n",
    "### Reguła decyzyjna w regresji logistycznej\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "  \\hat{y_i} =  & \\ \\ \\ \\ 1, \\ \\ \\text{ gdy }\\frac{1}{1+e^{-\\beta x_i}} \\geq 0.5, \\\\\n",
    "  \\hat{y_i} = & - 1, \\ \\ \\text{ gdy } \\frac{1}{1+e^{-\\beta x_i}} < 0.5,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Jest ona równoważna regule:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "  \\hat{y_i} =  & \\ \\ \\ \\ 1, \\ \\ \\text{ gdy } \\beta x_i \\geq 0, \\\\\n",
    "  \\hat{y_i} = & - 1, \\ \\ \\text{ gdy } \\beta x_i < 0.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Czyli oba klasyfikatory mają identyczną postać reguły decyzyjnej: __liniową__.\n",
    "\n",
    "Klasyfikatory o takiej postaci reguły decyzyjnej nazywają się klasyfikatorami liniowymi. Różnią się one sposobem wyznaczania hiperpłaszczyzny (np. prostej w przypadku 2D) rozdzielającej.\n",
    "\n",
    "Uwaga: pomimo tego, że są to klasyfikatory liniowe, to można objąć nimi nieliniowość (transformacje zmiennych, czy kernel trick w svm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadanie:\n",
    "\n",
    "Skopiuj kod z :\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
    "\n",
    "Zmodyfikuj go tak, aby wyswietli funkcje decyzyjne dla różnych wariantów klasyfikatora SVM z jądrem radialnym:\n",
    "\n",
    "a) dla gamma = 2 i wartości C: [0.0001, 0.1, 10, 10000]\n",
    "\n",
    "b) dla C = 0.1 i wartości gamma: [0.0001, 0.1, 10, 10000]\n",
    "\n",
    "Zinterpretuj wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównaj działania klasyfikatora SVM z domyslnymi parametrami na danych digits ze standaryzacją i bez standaryzacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoptymalizuj i przetestuj klasyfikator SVM na danych digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasy losowe\n",
    "\n",
    "Wyobraźmy sobie, że mamy 100 klasyfikatorów i każdy z nich potrafi przewidywać $Y$ ze skutecznością $70\\%$. Pytanie: jaką skuteczność będzie miała procedura klasyfikacyjna polegająca na dokonaniu predykcji każdym klasyfikatorem, a następnie podjęcie ostatecznej decyzji demokratycznie - czyli finalna decyzja to klasa dominująca wśród predykcji tych stu klasyfikatorów?\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Jeżeli każdy klasyfikator ma skuteczność $70\\%$, to średnio 70 ze 100 klasyfikatorów podejmie prawidłową decyzję. Jakie jest prawdopodobieństwo, że więcej niż 50 się pomyli? \n",
    "\n",
    "Bardzo małe... Zatem mamy doskonałą metodę klasyfikacji: nauczmy dużo modeli i klasyfikujmy demokratycznie.\n",
    "\n",
    "Zatem po co ogóle uczyć się uczenia maszynowego, skoro możemy załatwić wszystko w ten prosty sposób?\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Po to, bo jest jeden haczyk: **niezależność**...\n",
    "\n",
    "Tak pięknie byłoby, gdyby te wszystkie klasyfikatory były niezależne. A nie są... Rozważmy sytuację, w które nauczyliśmy dwa modele: regresję logistyczną i SVM. Na czym polega zależność między tymi klasyfikatorami?\n",
    "\n",
    "1. Oba klasyfikatory uczone są na tych samych zmiennych - zatem wykryją podobne zależności w danych, co przekłada się na podobieństwo predykcji.\n",
    "\n",
    "2. Oba klasyfikatory są nauczone na tych samych obserwacjach.\n",
    "\n",
    "Ponadto w tym konkretnym przypadku oba klasyfikatory są to klasyfikatory liniowe, zatem podejmują decyzję na podstawie płaszczyzny rozdzielającej. Te płaszczyzny mogą być bardzo podobne... A nawet jeśli nie są bardzo podobne, to i tak predykcje obu klasyfikatorów w praktyce będą silnie skorelowane, tzn. oba klasyfikatory będą dawały bardzo podobne predykcje...\n",
    "\n",
    "Co możemy na to poradzić?\n",
    "\n",
    "- każdy klasyfikator uczyć na innych zmiennych -> w praktyce losowe podzbiory, \n",
    "- każdy klasyfikator uczyć na innym podzbiorze obserwacji (w praktyce ma to mniejsze znaczenie).\n",
    "\n",
    "W obu przypadkach pojedyncze klasyfikatory będą słabsze (bo wykorzystują mniej informacji), ale globalnie to pomoże!\n",
    "\n",
    "Czy dwa powyższe zabiegi zapewniają niezależność klasyfikacji między poszczególnymi klasyfikatorami?\n",
    "\n",
    "Nie, bo:\n",
    "- nawet jak różne klasyfikatory uczymy na różnych zmiennych, to te zmienne będą z reguły zależne. Do tego w praktyce i tak nie jesteśmy w stanie nauczyć poszczególnych modeli na rozłącznych zbiorach zmiennych, bo musielibyśmy mieć tych zmiennych bardzo dużo (no chyba, że w każdym modelu wykorzystamy bardzo mało zmiennych, ale wtedy te modele będą słabe),\n",
    "- nawet jak uczymy na różnych obserwacjach, to i tak w tych obserwacjach będą ukryte te same wzorce, które będą wykrywały modele.\n",
    "\n",
    "\n",
    "### Lasy losowe - komitet drzew uczonych na różnych podzbiorach zmiennych i (ewentualnie) obserwacji.\n",
    "\n",
    "Na marginesie: ciekawostką jest, że w praktyce przyjęło się, że mówiąc \"lasy losowe\" mamy na myśli **jeden** klasyfikator, czyli tak naprawdę \"LAS losowy\".\n",
    "\n",
    "W sklearn las losowy zdefiniowany jest trochę inaczej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W sklearnie drzewa nie są uczone na podzbiorach zmiennych. Randomizacja jest realizowana przez próbkowanie obserwacji (i ewentualnie ograniczanie przeszukiwania podziałów do losowego podzbioru zmiennych o zadanej liczbie).\n",
    "\n",
    "Opisaną powyżej wersję lasu losowego można otrzymać wykorzystując `BaggingClassifier` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "?BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "Zrealizować przy użyciu tej klasy las losowy o \"podręcznikowej\" definicji:\n",
    "- każde drzewo uczone jest na losowym podzbiorze zmiennych (np połowie),\n",
    "- każde drzewo uczone jest na losowym podzbiorze obserwacji (np. połowie),\n",
    "- obserwacje losowane są bez powtórzeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "Narysuje wykres skuteczności lasu od liczby drzew na danych digits. Wyciągnij 3 wnioski."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie\n",
    "\n",
    "Narysuj wykres skrzynkowy przedstawiający rozkład skuteczności drzew w lesie losowym (po dopasowaniu lasu losowego wyciągnij z niego każde drzewo i oceń jest na zbiorze testowym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Titanic\n",
    "\n",
    "Znaleźć najlepszy klasyfikator, który będzie przewidywał czy dana osoba przetrwa.\n",
    "\n",
    "(przyda się funkcja pd.get_dummies())\n",
    "\n",
    "\n",
    "Cel: porównać wszystkie poznane klasyfikatory.\n",
    "\n",
    "Tak jak poprzednio na wejściu przyjmujemy listę modeli/pipelinów i siatek parametrów dla każdego modelu i chcemy zaimplementować analizę zupełnie automatycznie\n",
    "\n",
    "Elementy zadania:\n",
    "- przygotować dane\n",
    "- wszystkie klasyfikatory zoptymalizować na części treningowej\n",
    "- w optymalizacji svm'a nie wykonywać niepotrzebnych obliczeń (różne jądra korzystają z różnych parameterów)\n",
    "- w baggingu zoptymalizować parametry drzewa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbing, Mr. Anthony</td>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A. 5547</td>\n",
       "      <td>7.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbott, Master. Eugene Joseph</td>\n",
       "      <td>male</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>C.A. 2673</td>\n",
       "      <td>20.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>East Providence, RI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Abbott, Mr. Rossmore Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C.A. 2673</td>\n",
       "      <td>20.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>East Providence, RI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Abbott, Mrs. Stanton (Rosa Hunt)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C.A. 2673</td>\n",
       "      <td>20.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>East Providence, RI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Abelseth, Miss. Karen Marie</td>\n",
       "      <td>female</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>348125</td>\n",
       "      <td>7.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Norway Los Angeles, CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                              name     sex   age  sibsp  \\\n",
       "0       3         0               Abbing, Mr. Anthony    male  42.0      0   \n",
       "1       3         0     Abbott, Master. Eugene Joseph    male  13.0      0   \n",
       "2       3         0       Abbott, Mr. Rossmore Edward    male  16.0      1   \n",
       "3       3         1  Abbott, Mrs. Stanton (Rosa Hunt)  female  35.0      1   \n",
       "4       3         1       Abelseth, Miss. Karen Marie  female  16.0      0   \n",
       "\n",
       "   parch     ticket   fare cabin embarked               home.dest  \n",
       "0      0  C.A. 5547   7.55   NaN        S                     NaN  \n",
       "1      2  C.A. 2673  20.25   NaN        S     East Providence, RI  \n",
       "2      1  C.A. 2673  20.25   NaN        S     East Providence, RI  \n",
       "3      1  C.A. 2673  20.25   NaN        S     East Providence, RI  \n",
       "4      0     348125   7.65   NaN        S  Norway Los Angeles, CA  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Dane/titanic.csv\",decimal=\",\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omówienie projektów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
